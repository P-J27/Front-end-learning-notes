

#### 填空题

##### 欧几里得距离（欧氏距离）

> 例题：测试样本，属性为：[1,0,2]，训练样本，属性为：[2,0,2]，求测试样本到训练样本的欧氏距离。
>
> 答案：1

**公式参考**

![image-20210626004034450](https://gitee.com/p_pj/picgo/raw/master/img/20210626004035.png)

##### 余弦相似度

> 例题：x=[3,2,0,5,0,0,0,2,0,0],y=[1,0,0,0,0,0,0,1,0,2],求两者之间的夹角余弦相似度
>
> 答案：0.31
>
> ![image-20210626005550988](https://gitee.com/p_pj/picgo/raw/master/img/20210626005611.png)

**公式参考**

![image-20210626004246091](https://gitee.com/p_pj/picgo/raw/master/img/20210626004246.png)



---------------------

![image-20210626005842755](https://gitee.com/p_pj/picgo/raw/master/img/20210626005921.png)

##### 简单匹配系数

> 例题：x=[0,1,1,0,0],y=[1,1,0,0,1],求两者之间的简单匹配系数
>
> 答案：0.4

**公式参考**

![image-20210626005957729](https://gitee.com/p_pj/picgo/raw/master/img/20210626005957.png)

##### Jaccard系数

> 例题：x=[0,1,1,0,0],y=[1,1,0,0,1],求两者之间的Jaccard相似性系数
>
> 答案：0.25

**公式参考**

![image-20210626010120523](https://gitee.com/p_pj/picgo/raw/master/img/20210626010120.png)



----------------------



##### 数据集的Classification Error

> 例题：已知一个数据集，其中有3个类的样本，这3个类的样本数量分别为1、1、3，求Classification Error
>
> 答案： 0.4
>
> 解答：![img](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAQYAAAAbCAIAAAARaLaaAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAZ0SURBVHhe7Zx9bFNVFMBvjaMFwsSPDZPBmLaR6eYHyoixi6GNEYhRMBBRDNkIVIMS7UwAAwyJBDX4sWKII7YTFokZ+0fiMBoje7JQQPkSYzFbVlwWSPhwUwZL15ZZ7333vL771tf2+UZf38L9/bG+cz/ePe/2nnvOuW1nSSQSiMPhSNwCrxwOR4SbRFqG/vx+/ZzitQKIGYhfOh5YXjkvEAY5DRc73pnrKLZZLLbiOe8d6YdSQzGBCqaHm4Q6PV9v/Lhz3J2Ry3EoSMu1ozvrv70+sSB0DQrScq3rN8eO368mYp3bJmxoPQ2lhmICFcwPziU4aej2O5G3HYTMtHuR098NQkYi/X+0rlr62ekrIOcBE6hgYriXMJRwoHr8HY+8fma64/bxUGQ0JlDB3HCTMBT7ykOJ2OUDLx59tmZPD5QZjAlUMDfcJBQIdZa6ZD4dj0biaDA6BGImhqKDKB6JZk48hLrSOqEfFdimTJmKbptA9mg8XnW2rBzoPxVYgvuDpBMVFUZDvGvv8spCi8VSOHXxp6fSZ+v9eFxmYk0OBFBpIfE0i8aI+YaBg3SMxoh+lLC5Ax2XkvmZFTOUSdG+k/7aikkIWYtm1rZ0xsQy3Dn7jMYudmx9HHdEsxs7oUgnairo5srBNdOszzefjSQioUaX1bqsrQ9qlPT98No0q9Wg93D0aDIJo80AENeat50sTkOmk7UIw8iel1/ev652TyiSxzdCnStty1DR5p+pEGv3WtHClgtUYsEG4fqwrTEPU6sTnYGTGGDgPxji+HHKVh0QAtVEpv4RlxBBJOkxR/TKBol6Ew0ukNQRb5kcjNyV3h8jO+pkkTQsKZA0IJ3FpmFhd9BZUSoWGofrOW9wt5BpMu565oNdLz9gA8k8/NrRiipLi6lQUFoxC313opNKMv37ve8/uPPN+28FeQygP5fwuesryJ51aKWdiEGPO7SFGBlew3iVOTxVsCm0e31u2QDkXogxGkBftOlz16BmOlLQ47DQ++M91eemKz1QTYtIIfLUEFVcDaQtuQwHajzIv5rYXW8oiKrs4sMYSWmFMxjqxRfsLgLomxD9qKggkbqHDcej6OGyMpCK7r4XRePDIAHn96745ImdK+4rAHlsIK6VDIjRiwx4vxHuXhlyjAhAZI+fPUhQI2PgpLglK8ijMrC3ItdOp9yI6QDPyiC2UAGqGaBCDWjBIBYrHiE9qg9EIY+SBsXEQRkDVOiCnUzMCBFzrmWhq5EmLHRJxHp2bdDwpHlGk5dg3okscYwI2W4Z7PYquLoBMPuY5i1U7uP2QRHGtdrvDAaD3i3Uy7HAszJARQpQzQAVakALBqgYLa4GuF8qivcLyhigQjfHwpLvCIePoaLJE0EidGyv2SesmjGOTLzDE8TefFzZ8p+g0sTk4hAWhwJwJUImSwV9cYKYX1C0WCdGqGODOCgkw+OIyet1+upTIoI8MdYCp3secqHjoV568By/1NOF5j1WLgqUJ7cN0VnHUC+BLyDMNjO5MAniFuSVJuzwBJ21rpSZYBa3hMZF/r8gBiklzcI3SS+BtUL+5oaGZim9ELWmQX1mSGqubalqa0kUFFOYLBMyfD0ur8AcoaKCROpaLnv61YWocUvTmSE0dKZpw0e2zavnFuJy9uxCYvCfv1Df4ABI5kaTSZCsVSJ1t0gFO/L2KqmP+5i/W9/eAHsWiXZ8bnKlZ9O0rySrnupSj7zUfwl1bh+NmGi1aBTEucmBgGHgMDPLOVdPy9Ly8qmzVv2Cor75JY7y8q2HoSa/lCxpEjb9u+nR8ZbJcz6f3vzj+tlp0ujDm5/a2Im+XPzC2Pi4HDYBDuPdM3Fuz/xpaw5q+cKctpapWSknv3CTYGBtgkk7mNOFqwfWLfpixKe+pJeEvLg1tjTaItSV5TBwk1CAV2hyrWpdMd1+r8aTxdSWeDwtB7A3Du3K3rTkIr0ew+AsKBc5flrweGPgDObmgptEWpoWkO942ooX7OrKcsxz+q2ZNty0sHK9kOW3m9pb5gwTqGBywFtwRhAbGIjgl8j5r5aof51NJjIwgHOG2MCRd2ckvwanjvaWOcMEKpgc7iXUEdbO331Wyy8lUDjw0tvBv7V8XKC9Zc4wgQrmB0yDo+REw+wS8gsFa9HMN9p6lQdHSi7se8VeZMVNJ5Us2n5S/RcDFO0tc4YJVDA9/F+bcTgKeODE4TAg9B9OYnffVa6E/QAAAABJRU5ErkJggg==)

**公式参考**

![image-20210626010620383](https://gitee.com/p_pj/picgo/raw/master/img/20210626010620.png)

##### 数据集的GINI（基尼系数）

> 例题：已知一个数据集，其中有2个类的样本，这2个类的样本数量分别为1、3，求该数据集的GINI。
>
> 答案： 0.375
>
> 解答：![img](data:image/png;base64,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)

**参考公式**

![image-20210626011227442](https://gitee.com/p_pj/picgo/raw/master/img/20210626011227.png)

---------------



##### 召回率

> 例题：TP=90，FN=20，TN=120，FP=10，计算其召回率
>
> 答案： 9/11

**公式参考**

![](https://gitee.com/p_pj/picgo/raw/master/img/20210626101732.png)

##### 精度

> 例题：TP=90，FN=20，TN=120，FP=10，求计算其精度
>
> 答案： 9/10

**公式**

![](https://gitee.com/p_pj/picgo/raw/master/img/20210626101803.png)

---------------------



#### 问答题

##### 简述支持向量机的“最大边缘”原理

即追求分类器的泛化能力最大化。即希望所找到的决策边界，在满足将两类数据点正确的分开的前提下，对应的分类器边缘最大。这样可以使得新的测试数据被错分的几率尽可能小。

##### 简述软边缘支持向量机的基本工作原理

对存在数据污染、近似线性分类的情况，可能并不存在一个最优的线性决策超平面；当存在噪声数据时，为保证所有训练数据的准确分类，可能会导致过拟合。因此，需要允许有一定程度“错分”，又有较大分界区域的最优决策超平面，即软间隔支持向量机。
软间隔支持向量机通过引入松弛变量、惩罚因子，在一定程度上允许错误分类样本，以增大间隔距离。在分类准确性与泛化能力上寻求一个平衡点。

##### 简述非线性支持向量机的基本工作原理

对非线性可分的问题，可以利用核变换，把原样本映射到某个高维特征空间，使得原本在低维特征空间中非线性可分的样本，在新的高维特征空间中变得线性可分，并使用线性支持向量机进行分类。

---------------

#### 计算题

##### 朴素贝叶斯分类

![image-20210626105424691](https://gitee.com/p_pj/picgo/raw/master/img/20210626105424.png)

**问题**

1. 已知训练数据集如上图：该数据集中，求P(yes)， P(no)
2. 已知待分类的测试样本X=（有房=否，婚姻=已婚）



**参考步骤**

![image-20210626110344908](https://gitee.com/p_pj/picgo/raw/master/img/20210626110344.png)





**答案参考**

![image-20210626110249630](https://gitee.com/p_pj/picgo/raw/master/img/20210626110249.png)



--------------------

##### ID3决策树，计算数据集的熵、期望和信息增益

![image-20210626112306038](https://gitee.com/p_pj/picgo/raw/master/img/20210626112306.png)

**熵公式参考**

![](https://gitee.com/p_pj/picgo/raw/master/img/20210626112520.png)

1. 求该数据集的熵为 Info(D)。

   > P(yes)=0.6，P(no)= 0.4
   >
   > Info(D) = –(3/5)log2(3/5)–(2/5)log2(2/5)

2. 以Attribute1为分裂属性，将数据集分成三个子集D1、D2、D3，分别对应Attribute1=V1a，Attribute1=V1b，Attribute1=V1c。求这三个子集的熵

   > 计算三个子集的样本数量与原始数据集的比例
   >
   > D1:0.4   D2:0.2  D3:0.4
   >
   > 求D1的熵。
   >
   > P(yes)=1/4，P(no)= 3/4
   >
   > Info(D1)=– (1/4)log2(1/4)–(3/4)log2(3/4)
   >
   > 同理
   >
   > Info(D2) = – 1log2(1)
   >
   > Info(D3) = – (1/4)log2(1/4)–(3/4)log2(3/4)

--------------

**期望信息公式参考**

![image-20210626113613762](https://gitee.com/p_pj/picgo/raw/master/img/20210626113613.png)

1. 以Attribute1为分裂属性，将数据集分成三个子集D1、D2、D3，分别对应Attribute1=V1a，Attribute1=V1b，Attribute1=V1c。求该划分的期望信息

   > InfoA（D） = P(D1)xInfo(D1)+P(D1)xInfo(D1)+P(D1)xInfo(D1)
   >
   > InfoA (D) =![image-20210626170037677](https://gitee.com/p_pj/picgo/raw/master/img/20210626170037.png)
   >
   > 提示：![image-20210626170520402](https://gitee.com/p_pj/picgo/raw/master/img/20210626170520.png)

   

2. 在上题的基础上，求该划分的信息增益

   > Gain（A）= Info（D）- InfoA（D）
   >
   > 
   >
   > ![image-20210626170842522](https://gitee.com/p_pj/picgo/raw/master/img/20210626171007.png)

   

   **信息增益公式参考**

   ![image-20210626170745619](https://gitee.com/p_pj/picgo/raw/master/img/20210626170745.png)

   ---------------------

##### 欧氏距离和KNN分类

已知有5个训练样本，分别为：
样本1，属性为：[2,0,2] 类别 0
样本2，属性为：[1,5,2] 类别 1
样本3，属性为：[3,2,3] 类别 1
样本4，属性为：[3,0,2] 类别 0
样本5，属性为：[1,0,6] 类别 0

有1个测试样本，属性为：[1,0,2]

（1）测试样本到5个训练样本（样本1、2、3、4、5）的欧氏距离依次为：1、5、3、2、4

（2）K=3，距离测试样本最近的k个训练样本依次为：样本1、样本4、样本3

（3）距离最近的k个训练样本类别依次为：类别0、类别0、类别1

（4）KNN算法得到的测试样本的类别为： 类别0

`注意`：具体欧氏距离参考上面填空做法，KNN测试样本类别，选择K个中出现频率最高的。

------------------------



##### 求给定数据集的频繁K项集、指定关联规则的支持度及置信度

**公式参考**

![image-20210626173622144](https://gitee.com/p_pj/picgo/raw/master/img/20210626173622.png)

**例题**

![image-20210626173349552](https://gitee.com/p_pj/picgo/raw/master/img/20210626173349.png)

已知购物篮数据如下表所示，回答以下问题。

（1）计算所有2项集及其支持度。

```
{Bread，Mike}：s = 3/5
{Bread，Diaper}：s = 4/5
{Bread，Beer}：s = 2/5
{Diaper，Mike}：s = 4/5
{Beer，Mike}：s = 2/5
{Diaper，Mike}：s = 3/5
```

（2）给定最小支持度阈值为2/5，列出所有频繁2项集。

```
所有的S都大于最小支持度阈值
{Bread，Mike}{Bread，Diaper}{Bread，Beer}{Diaper，Mike}{Beer，Mike}{Diaper，Mike}
```

（3）关联规则X->Y的支持度计算公式是什么？

![image-20210626174332631](https://gitee.com/p_pj/picgo/raw/master/img/20210626174332.png)

（4）关联规则X->Y的置信度计算公式是什么？

![image-20210626174344028](https://gitee.com/p_pj/picgo/raw/master/img/20210626174344.png)

（5）计算规则{Milk, Bread }->{ Diaper }的支持度和置信度。

```
即求{Milk, Bread，Diaper }的支持度. 
其支持度计数为3，事务总数为5.
故支持度s({Milk, Bread，Diaper }) = 3/5.
{Milk, Bread }的支持度计数为3.
所以{Milk, Bread }->{ Diaper }的置信度c=3/3 = 1
```

